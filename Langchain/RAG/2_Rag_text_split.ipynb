{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -r ../requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: langchain-openai in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (0.1.21)\nRequirement already satisfied: python-dotenv==1.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (1.0.1)\nRequirement already satisfied: langchain in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (0.2.13)\nRequirement already satisfied: wikipedia==1.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (1.4.0)\nRequirement already satisfied: tavily-python in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (0.3.9)\nRequirement already satisfied: sentence-transformers in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (3.0.1)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (4.66.4)\nRequirement already satisfied: beautifulsoup4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from wikipedia==1.4.0->-r ../requirements.txt (line 4)) (4.12.3)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from wikipedia==1.4.0->-r ../requirements.txt (line 4)) (2.32.3)\nRequirement already satisfied: langchain-core<0.3.0,>=0.2.29 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-openai->-r ../requirements.txt (line 1)) (0.2.30)\nRequirement already satisfied: openai<2.0.0,>=1.40.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-openai->-r ../requirements.txt (line 1)) (1.40.3)\nRequirement already satisfied: tiktoken<1,>=0.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-openai->-r ../requirements.txt (line 1)) (0.7.0)\nRequirement already satisfied: PyYAML>=5.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (3.10.3)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (4.0.3)\nRequirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (0.2.2)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (0.1.98)\nRequirement already satisfied: numpy<2,>=1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (1.23.5)\nRequirement already satisfied: pydantic<3,>=1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain->-r ../requirements.txt (line 3)) (8.5.0)\nRequirement already satisfied: httpx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tavily-python->-r ../requirements.txt (line 5)) (0.27.0)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 6)) (4.44.2)\nRequirement already satisfied: torch>=1.11.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 6)) (2.4.1)\nRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 6)) (1.5.0)\nRequirement already satisfied: scipy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 6)) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 6)) (0.24.6)\nRequirement already satisfied: Pillow in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sentence-transformers->-r ../requirements.txt (line 6)) (10.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 3)) (2.3.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 3)) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 3)) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 3)) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 3)) (1.9.4)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r ../requirements.txt (line 6)) (3.14.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r ../requirements.txt (line 6)) (2023.10.0)\nRequirement already satisfied: packaging>=20.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r ../requirements.txt (line 6)) (24.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->-r ../requirements.txt (line 6)) (4.12.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain-openai->-r ../requirements.txt (line 1)) (1.33)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r ../requirements.txt (line 3)) (3.10.7)\nRequirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai->-r ../requirements.txt (line 1)) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai->-r ../requirements.txt (line 1)) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai->-r ../requirements.txt (line 1)) (0.5.0)\nRequirement already satisfied: sniffio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai->-r ../requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: certifi in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx->tavily-python->-r ../requirements.txt (line 5)) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx->tavily-python->-r ../requirements.txt (line 5)) (1.0.5)\nRequirement already satisfied: idna in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx->tavily-python->-r ../requirements.txt (line 5)) (3.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx->tavily-python->-r ../requirements.txt (line 5)) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3,>=1->langchain->-r ../requirements.txt (line 3)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3,>=1->langchain->-r ../requirements.txt (line 3)) (2.20.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0->-r ../requirements.txt (line 4)) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0->-r ../requirements.txt (line 4)) (2.2.1)\nRequirement already satisfied: greenlet!=0.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r ../requirements.txt (line 3)) (3.0.3)\nRequirement already satisfied: regex>=2022.1.18 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai->-r ../requirements.txt (line 1)) (2024.7.24)\nRequirement already satisfied: sympy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (1.13.2)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (3.3)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (12.6.68)\nRequirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->-r ../requirements.txt (line 6)) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->-r ../requirements.txt (line 6)) (0.19.1)\nRequirement already satisfied: soupsieve>1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from beautifulsoup4->wikipedia==1.4.0->-r ../requirements.txt (line 4)) (2.5)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r ../requirements.txt (line 6)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r ../requirements.txt (line 6)) (3.5.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai->-r ../requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: jsonpointer>=1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.29->langchain-openai->-r ../requirements.txt (line 1)) (3.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 6)) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Env Variables and Secrets"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('../../../azure.env')\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-06-01\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = 'gpt-4o-mini'\n",
        "os.environ[\"AZURE_OPENAI_MODEL_VERSION\"] = '2024-06-01'\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1725999789498
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    SentenceTransformersTokenTextSplitter,\n",
        "    TextSplitter,\n",
        "    TokenTextSplitter,\n",
        ")\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_core.vectorstores import InMemoryVectorStore"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999790402
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AzureChatOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "    model_version=os.environ['AZURE_OPENAI_MODEL_VERSION']\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1725999790472
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "    # dimensions: Optional[int] = None, # Can specify dimensions with new text-embedding-3 models\n",
        "    # azure_endpoint=\"https://<your-endpoint>.openai.azure.com/\", If not provided, will read env variable AZURE_OPENAI_ENDPOINT\n",
        "    # api_key=... # Can provide an API key directly. If missing read env variable AZURE_OPENAI_API_KEY\n",
        "    # openai_api_version=..., # If not provided, will read env variable AZURE_OPENAI_API_VERSION\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999790546
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory containing the text file and the persistent directory\n",
        "current_dir = os.path.dirname(os.path.commonpath('.'))\n",
        "books_dir = os.path.join(current_dir, \"books_small\")\n",
        "print(f\"Books directory: {books_dir}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Books directory: books_small\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999790608
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the books directory exists\n",
        "if not os.path.exists(books_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"The directory {books_dir} does not exist. Please check the path.\"\n",
        "    )\n",
        "\n",
        "# List all text files in the directory\n",
        "book_files = [f for f in os.listdir(books_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "# Read the text content from each file and store it with metadata\n",
        "documents = []\n",
        "for book_file in book_files:\n",
        "    file_path = os.path.join(books_dir, book_file)\n",
        "    loader = TextLoader(file_path)\n",
        "    book_docs = loader.load()\n",
        "    for doc in book_docs:\n",
        "        # Add metadata to each document indicating its source\n",
        "        doc.metadata = {\"source\": book_file}\n",
        "        documents.append(doc)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999790676
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create and persist vector store\n",
        "def create_vector_store(docs, store_name):\n",
        "    # Create the vector store and persist it automatically\n",
        "    print(f\"\\n--- Creating vector store {store_name} ---\")\n",
        "    vectorstore = InMemoryVectorStore.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "    )\n",
        "    print(\"\\n--- Finished creating vector store ---\")\n",
        "    return vectorstore"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999790734
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query a vector store\n",
        "def query_vector_store(vcs,store_name, query):\n",
        "    print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "    # Use the vectorstore as a retriever\n",
        "    retriever = vcs.as_retriever(\n",
        "        search_kwargs={'k': 1,'score_threshold': 0.8})\n",
        "\n",
        "    # Retrieve the most similar text\n",
        "    retrieved_documents = retriever.invoke(query)\n",
        "\n",
        "    # Display the relevant results with metadata\n",
        "    print(\"\\n--- Relevant Documents ---\")\n",
        "    for i, doc in enumerate(retrieved_documents, 1):\n",
        "        print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "        if doc.metadata:\n",
        "            print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999790802
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the user's question\n",
        "query = \"How did Juliet die?\""
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999790861
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Character-based Splitting\n",
        "# Splits text into chunks based on a specified number of characters.\n",
        "# Useful for consistent chunk sizes regardless of content structure.\n",
        "print(\"\\n--- Using Character-based Splitting ---\")\n",
        "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "char_docs = char_splitter.split_documents(documents)\n",
        "vcs = create_vector_store(char_docs, \"db_char\")\n",
        "query_vector_store(vcs,\"db_char\", query)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Created a chunk of size 1141, which is longer than the specified 1000\nCreated a chunk of size 2086, which is longer than the specified 1000\nCreated a chunk of size 1121, which is longer than the specified 1000\nCreated a chunk of size 1366, which is longer than the specified 1000\nCreated a chunk of size 1011, which is longer than the specified 1000\nCreated a chunk of size 1639, which is longer than the specified 1000\nCreated a chunk of size 1219, which is longer than the specified 1000\nCreated a chunk of size 1875, which is longer than the specified 1000\nCreated a chunk of size 1307, which is longer than the specified 1000\nCreated a chunk of size 2271, which is longer than the specified 1000\nCreated a chunk of size 1430, which is longer than the specified 1000\nCreated a chunk of size 1763, which is longer than the specified 1000\nCreated a chunk of size 1575, which is longer than the specified 1000\nCreated a chunk of size 1028, which is longer than the specified 1000\nCreated a chunk of size 1149, which is longer than the specified 1000\nCreated a chunk of size 1199, which is longer than the specified 1000\nCreated a chunk of size 1604, which is longer than the specified 1000\nCreated a chunk of size 1832, which is longer than the specified 1000\nCreated a chunk of size 1093, which is longer than the specified 1000\nCreated a chunk of size 1995, which is longer than the specified 1000\nCreated a chunk of size 1463, which is longer than the specified 1000\nCreated a chunk of size 1098, which is longer than the specified 1000\nCreated a chunk of size 1053, which is longer than the specified 1000\nCreated a chunk of size 1027, which is longer than the specified 1000\nCreated a chunk of size 1315, which is longer than the specified 1000\nCreated a chunk of size 1627, which is longer than the specified 1000\nCreated a chunk of size 1063, which is longer than the specified 1000\nCreated a chunk of size 1051, which is longer than the specified 1000\nCreated a chunk of size 1265, which is longer than the specified 1000\nCreated a chunk of size 1202, which is longer than the specified 1000\nCreated a chunk of size 1509, which is longer than the specified 1000\nCreated a chunk of size 1153, which is longer than the specified 1000\nCreated a chunk of size 1043, which is longer than the specified 1000\nCreated a chunk of size 1336, which is longer than the specified 1000\nCreated a chunk of size 1111, which is longer than the specified 1000\nCreated a chunk of size 1205, which is longer than the specified 1000\nCreated a chunk of size 1158, which is longer than the specified 1000\nCreated a chunk of size 1004, which is longer than the specified 1000\nCreated a chunk of size 1155, which is longer than the specified 1000\nCreated a chunk of size 1085, which is longer than the specified 1000\nCreated a chunk of size 1064, which is longer than the specified 1000\nCreated a chunk of size 1078, which is longer than the specified 1000\nCreated a chunk of size 1827, which is longer than the specified 1000\nCreated a chunk of size 1188, which is longer than the specified 1000\nCreated a chunk of size 1058, which is longer than the specified 1000\nCreated a chunk of size 1226, which is longer than the specified 1000\nCreated a chunk of size 1419, which is longer than the specified 1000\nCreated a chunk of size 1223, which is longer than the specified 1000\nCreated a chunk of size 1284, which is longer than the specified 1000\nCreated a chunk of size 1112, which is longer than the specified 1000\nCreated a chunk of size 1437, which is longer than the specified 1000\nCreated a chunk of size 1030, which is longer than the specified 1000\nCreated a chunk of size 1094, which is longer than the specified 1000\nCreated a chunk of size 1079, which is longer than the specified 1000\nCreated a chunk of size 1073, which is longer than the specified 1000\nCreated a chunk of size 1550, which is longer than the specified 1000\nCreated a chunk of size 1737, which is longer than the specified 1000\nCreated a chunk of size 1038, which is longer than the specified 1000\nCreated a chunk of size 1011, which is longer than the specified 1000\nCreated a chunk of size 1382, which is longer than the specified 1000\nCreated a chunk of size 1184, which is longer than the specified 1000\nCreated a chunk of size 1378, which is longer than the specified 1000\nCreated a chunk of size 1052, which is longer than the specified 1000\nCreated a chunk of size 1402, which is longer than the specified 1000\nCreated a chunk of size 1047, which is longer than the specified 1000\nCreated a chunk of size 1163, which is longer than the specified 1000\nCreated a chunk of size 1403, which is longer than the specified 1000\nCreated a chunk of size 1077, which is longer than the specified 1000\nCreated a chunk of size 1588, which is longer than the specified 1000\nCreated a chunk of size 1228, which is longer than the specified 1000\nCreated a chunk of size 1337, which is longer than the specified 1000\nCreated a chunk of size 1469, which is longer than the specified 1000\nCreated a chunk of size 1269, which is longer than the specified 1000\nCreated a chunk of size 1106, which is longer than the specified 1000\nCreated a chunk of size 1492, which is longer than the specified 1000\nCreated a chunk of size 1219, which is longer than the specified 1000\nCreated a chunk of size 1165, which is longer than the specified 1000\nCreated a chunk of size 1115, which is longer than the specified 1000\nCreated a chunk of size 1505, which is longer than the specified 1000\nCreated a chunk of size 1549, which is longer than the specified 1000\nCreated a chunk of size 1309, which is longer than the specified 1000\nCreated a chunk of size 1290, which is longer than the specified 1000\nCreated a chunk of size 1593, which is longer than the specified 1000\nCreated a chunk of size 1292, which is longer than the specified 1000\nCreated a chunk of size 1136, which is longer than the specified 1000\nCreated a chunk of size 1162, which is longer than the specified 1000\nCreated a chunk of size 1003, which is longer than the specified 1000\nCreated a chunk of size 1065, which is longer than the specified 1000\nCreated a chunk of size 1403, which is longer than the specified 1000\nCreated a chunk of size 1564, which is longer than the specified 1000\nCreated a chunk of size 1776, which is longer than the specified 1000\nCreated a chunk of size 1006, which is longer than the specified 1000\nCreated a chunk of size 1377, which is longer than the specified 1000\nCreated a chunk of size 1289, which is longer than the specified 1000\nCreated a chunk of size 1727, which is longer than the specified 1000\nCreated a chunk of size 1555, which is longer than the specified 1000\nCreated a chunk of size 1619, which is longer than the specified 1000\nCreated a chunk of size 1059, which is longer than the specified 1000\nCreated a chunk of size 1013, which is longer than the specified 1000\nCreated a chunk of size 1180, which is longer than the specified 1000\nCreated a chunk of size 1897, which is longer than the specified 1000\nCreated a chunk of size 1412, which is longer than the specified 1000\nCreated a chunk of size 1740, which is longer than the specified 1000\nCreated a chunk of size 1126, which is longer than the specified 1000\nCreated a chunk of size 1003, which is longer than the specified 1000\nCreated a chunk of size 1126, which is longer than the specified 1000\nCreated a chunk of size 1384, which is longer than the specified 1000\nCreated a chunk of size 1688, which is longer than the specified 1000\nCreated a chunk of size 1035, which is longer than the specified 1000\nCreated a chunk of size 2424, which is longer than the specified 1000\nCreated a chunk of size 2028, which is longer than the specified 1000\nCreated a chunk of size 1697, which is longer than the specified 1000\nCreated a chunk of size 2474, which is longer than the specified 1000\nCreated a chunk of size 1327, which is longer than the specified 1000\nCreated a chunk of size 1225, which is longer than the specified 1000\nCreated a chunk of size 1118, which is longer than the specified 1000\nCreated a chunk of size 1999, which is longer than the specified 1000\nCreated a chunk of size 2401, which is longer than the specified 1000\nCreated a chunk of size 1227, which is longer than the specified 1000\nCreated a chunk of size 1074, which is longer than the specified 1000\nCreated a chunk of size 1810, which is longer than the specified 1000\nCreated a chunk of size 1065, which is longer than the specified 1000\nCreated a chunk of size 1528, which is longer than the specified 1000\nCreated a chunk of size 1122, which is longer than the specified 1000\nCreated a chunk of size 2149, which is longer than the specified 1000\nCreated a chunk of size 1226, which is longer than the specified 1000\nCreated a chunk of size 1008, which is longer than the specified 1000\nCreated a chunk of size 1504, which is longer than the specified 1000\nCreated a chunk of size 1119, which is longer than the specified 1000\nCreated a chunk of size 1107, which is longer than the specified 1000\nCreated a chunk of size 1194, which is longer than the specified 1000\nCreated a chunk of size 1031, which is longer than the specified 1000\nCreated a chunk of size 1034, which is longer than the specified 1000\nCreated a chunk of size 1386, which is longer than the specified 1000\nCreated a chunk of size 1235, which is longer than the specified 1000\nCreated a chunk of size 1004, which is longer than the specified 1000\nCreated a chunk of size 1792, which is longer than the specified 1000\nCreated a chunk of size 1132, which is longer than the specified 1000\nCreated a chunk of size 1055, which is longer than the specified 1000\nCreated a chunk of size 1189, which is longer than the specified 1000\nCreated a chunk of size 1528, which is longer than the specified 1000\nCreated a chunk of size 2049, which is longer than the specified 1000\nCreated a chunk of size 1097, which is longer than the specified 1000\nCreated a chunk of size 1860, which is longer than the specified 1000\nCreated a chunk of size 1296, which is longer than the specified 1000\nCreated a chunk of size 1369, which is longer than the specified 1000\nCreated a chunk of size 1121, which is longer than the specified 1000\nCreated a chunk of size 1285, which is longer than the specified 1000\nCreated a chunk of size 1736, which is longer than the specified 1000\nCreated a chunk of size 1637, which is longer than the specified 1000\nCreated a chunk of size 1184, which is longer than the specified 1000\nCreated a chunk of size 1045, which is longer than the specified 1000\nCreated a chunk of size 1132, which is longer than the specified 1000\nCreated a chunk of size 1674, which is longer than the specified 1000\nCreated a chunk of size 1105, which is longer than the specified 1000\nCreated a chunk of size 1437, which is longer than the specified 1000\nCreated a chunk of size 1871, which is longer than the specified 1000\nCreated a chunk of size 1015, which is longer than the specified 1000\nCreated a chunk of size 1006, which is longer than the specified 1000\nCreated a chunk of size 1054, which is longer than the specified 1000\nCreated a chunk of size 1432, which is longer than the specified 1000\nCreated a chunk of size 1367, which is longer than the specified 1000\nCreated a chunk of size 2178, which is longer than the specified 1000\nCreated a chunk of size 1390, which is longer than the specified 1000\nCreated a chunk of size 1502, which is longer than the specified 1000\nCreated a chunk of size 1410, which is longer than the specified 1000\nCreated a chunk of size 1741, which is longer than the specified 1000\nCreated a chunk of size 1184, which is longer than the specified 1000\nCreated a chunk of size 1045, which is longer than the specified 1000\nCreated a chunk of size 1132, which is longer than the specified 1000\nCreated a chunk of size 1674, which is longer than the specified 1000\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n--- Using Character-based Splitting ---\n\n--- Creating vector store db_char ---\n\n--- Finished creating vector store ---\n\n--- Querying the Vector Store db_char ---\n\n--- Relevant Documents ---\nDocument 1:\nJULIET.\nShall I speak ill of him that is my husband?\nAh, poor my lord, what tongue shall smooth thy name,\nWhen I thy three-hours’ wife have mangled it?\nBut wherefore, villain, didst thou kill my cousin?\nThat villain cousin would have kill’d my husband.\nBack, foolish tears, back to your native spring,\nYour tributary drops belong to woe,\nWhich you mistaking offer up to joy.\nMy husband lives, that Tybalt would have slain,\nAnd Tybalt’s dead, that would have slain my husband.\nAll this is comfort; wherefore weep I then?\nSome word there was, worser than Tybalt’s death,\nThat murder’d me. I would forget it fain,\nBut O, it presses to my memory\nLike damned guilty deeds to sinners’ minds.\nTybalt is dead, and Romeo banished.\nThat ‘banished,’ that one word ‘banished,’\nHath slain ten thousand Tybalts. Tybalt’s death\nWas woe enough, if it had ended there.\nOr if sour woe delights in fellowship,\nAnd needly will be rank’d with other griefs,\nWhy follow’d not, when she said Tybalt’s dead,\nThy father or thy mother, nay or both,\nWhich modern lamentation might have mov’d?\nBut with a rear-ward following Tybalt’s death,\n‘Romeo is banished’—to speak that word\nIs father, mother, Tybalt, Romeo, Juliet,\nAll slain, all dead. Romeo is banished,\nThere is no end, no limit, measure, bound,\nIn that word’s death, no words can that woe sound.\nWhere is my father and my mother, Nurse?\n\nSource: romeo_and_juliet.txt\n\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999838205
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Sentence-based Splitting\n",
        "# Splits text into chunks based on sentences, ensuring chunks end at sentence boundaries.\n",
        "# Ideal for maintaining semantic coherence within chunks.\n",
        "print(\"\\n--- Using Sentence-based Splitting ---\")\n",
        "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
        "sent_docs = sent_splitter.split_documents(documents)\n",
        "vcs = create_vector_store(sent_docs, \"db_sent\")\n",
        "query_vector_store(vcs,\"db_sent\", query)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n--- Using Sentence-based Splitting ---\n\n--- Creating vector store db_sent ---\n\n--- Finished creating vector store ---\n\n--- Querying the Vector Store db_sent ---\n\n--- Relevant Documents ---\nDocument 1:\nam. where is my romeo? [ _ noise within. _ ] friar lawrence. i hear some noise. lady, come from that nest of death, contagion, and unnatural sleep. a greater power than we can contradict hath thwarted our intents. come, come away. thy husband in thy bosom there lies dead ; and paris too. come, i ’ ll dispose of thee among a sisterhood of holy nuns. stay not to question, for the watch is coming. come, go, good juliet. i dare no longer stay. juliet. go, get thee hence, for i will not away. [ _ exit friar lawrence. _ ] what ’ s here? a cup clos ’ d in my true love ’ s hand? poison, i see, hath been his timeless end. o churl. drink all, and left no friendly drop to help me after? i will kiss thy lips. haply some poison yet doth hang on them, to make me die with a restorative. [ _ kisses him. _ ] thy lips are warm! first watch. [ _ within. _ ] lead, boy. which way? juliet. yea, noise? then i ’ ll be brief. o happy dagger. [ _ snatching romeo ’ s dagger. _ ] this is thy sheath. [ _ stabs herself _ ] there rest, and let me die. [ _ falls on romeo ’ s body and dies. _ ] enter watch with the page of paris. page. this is the place. there, where the torch doth burn. first watch. the ground is bloody. search about the churchyard. go, some of you, whoe ’ er you find attach. [ _ exeunt some of the watch. _ ] pitiful sight! here lies the county slain, and juliet bleeding, warm, and newly dead, who\n\nSource: romeo_and_juliet.txt\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm, trange\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726000027698
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Token-based Splitting\n",
        "# Splits text into chunks based on tokens (words or subwords), using tokenizers like GPT-2.\n",
        "# Useful for transformer models with strict token limits.\n",
        "print(\"\\n--- Using Token-based Splitting ---\")\n",
        "token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
        "token_docs = token_splitter.split_documents(documents)\n",
        "vcs = create_vector_store(token_docs, \"db_token\")\n",
        "query_vector_store(vcs,\"db_token\", query)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n--- Using Token-based Splitting ---\n\n--- Creating vector store db_token ---\n\n--- Finished creating vector store ---\n\n--- Querying the Vector Store db_token ---\n\n--- Relevant Documents ---\nDocument 1:\nThe cords that Romeo bid thee fetch?\n\nNURSE.\nAy, ay, the cords.\n\n [_Throws them down._]\n\nJULIET.\nAy me, what news? Why dost thou wring thy hands?\n\nNURSE.\nAh, well-a-day, he’s dead, he’s dead, he’s dead!\nWe are undone, lady, we are undone.\nAlack the day, he’s gone, he’s kill’d, he’s dead.\n\nJULIET.\nCan heaven be so envious?\n\nNURSE.\nRomeo can,\nThough heaven cannot. O Romeo, Romeo.\nWho ever would have thought it? Romeo!\n\nJULIET.\nWhat devil art thou, that dost torment me thus?\nThis torture should be roar’d in dismal hell.\nHath Romeo slain himself? Say thou but Ay,\nAnd that bare vowel I shall poison more\nThan the death-darting eye of cockatrice.\nI am not I if there be such an I;\nOr those eyes shut that make thee answer Ay.\nIf he be slain, say Ay; or if not, No.\nBrief sounds determine of my weal or woe.\n\nNURSE.\nI saw the wound, I saw it with mine eyes,\nGod save the mark!—here on his manly breast.\nA piteous corse, a bloody piteous corse;\nPale, pale as ashes, all bedaub’d in blood,\nAll in gore-blood. I swounded at the sight.\n\nJULIET.\nO, break, my heart. Poor bankrout, break at once.\nTo prison, eyes; ne’er look on liberty.\nVile earth to earth resign; end motion here,\nAnd thou and Romeo press one heavy bier.\n\nNURSE.\nO Tybalt, Tybalt, the best friend I had.\nO courteous Tybalt, honest gentleman!\nThat ever I should live to see thee dead.\n\nJULIET.\nWhat storm is this that blows so contrary?\nIs Romeo slaughter’d and is Tybalt dead?\nMy\n\nSource: romeo_and_juliet.txt\n\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726000027782
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Recursive Character-based Splitting\n",
        "# Attempts to split text at natural boundaries (sentences, paragraphs) within character limit.\n",
        "# Balances between maintaining coherence and adhering to character limits.\n",
        "print(\"\\n--- Using Recursive Character-based Splitting ---\")\n",
        "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100)\n",
        "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
        "vcs = create_vector_store(rec_char_docs, \"db_rec_char\")\n",
        "query_vector_store(vcs,\"db_rec_char\", query)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n--- Using Recursive Character-based Splitting ---\n\n--- Creating vector store db_rec_char ---\n\n--- Finished creating vector store ---\n\n--- Querying the Vector Store db_rec_char ---\n\n--- Relevant Documents ---\nDocument 1:\nNURSE.\nI saw the wound, I saw it with mine eyes,\nGod save the mark!—here on his manly breast.\nA piteous corse, a bloody piteous corse;\nPale, pale as ashes, all bedaub’d in blood,\nAll in gore-blood. I swounded at the sight.\n\nJULIET.\nO, break, my heart. Poor bankrout, break at once.\nTo prison, eyes; ne’er look on liberty.\nVile earth to earth resign; end motion here,\nAnd thou and Romeo press one heavy bier.\n\nNURSE.\nO Tybalt, Tybalt, the best friend I had.\nO courteous Tybalt, honest gentleman!\nThat ever I should live to see thee dead.\n\nJULIET.\nWhat storm is this that blows so contrary?\nIs Romeo slaughter’d and is Tybalt dead?\nMy dearest cousin, and my dearer lord?\nThen dreadful trumpet sound the general doom,\nFor who is living, if those two are gone?\n\nNURSE.\nTybalt is gone, and Romeo banished,\nRomeo that kill’d him, he is banished.\n\nJULIET.\nO God! Did Romeo’s hand shed Tybalt’s blood?\n\nNURSE.\nIt did, it did; alas the day, it did.\n\nSource: romeo_and_juliet.txt\n\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726000027860
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Custom Splitting\n",
        "# Allows creating custom splitting logic based on specific requirements.\n",
        "# Useful for documents with unique structure that standard splitters can't handle.\n",
        "print(\"\\n--- Using Custom Splitting ---\")\n",
        "\n",
        "\n",
        "class CustomTextSplitter(TextSplitter):\n",
        "    def split_text(self, text):\n",
        "        # Custom logic for splitting text\n",
        "        return text.split(\"\\n\\n\")  # Example: split by paragraphs\n",
        "\n",
        "\n",
        "custom_splitter = CustomTextSplitter()\n",
        "custom_docs = custom_splitter.split_documents(documents)\n",
        "vcs = create_vector_store(custom_docs, \"db_custom\")\n",
        "query_vector_store(vcs,\"db_custom\", query)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n--- Using Custom Splitting ---\n\n--- Creating vector store db_custom ---\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725999778160
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}